{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import spacy\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_origin = pd.read_csv(\"train.csv\")\n",
    "data_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train,input_test,output_train,output_test = train_test_split(data_origin.description,data_origin.jobflag,test_size=0.2,random_state=0,stratify=data_origin.jobflag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "en_nlp = spacy.load('en')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "en_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(regexp.findall(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document)\n",
    "    return [token.lemma_ for token in doc_spacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masato\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: [W002] Tokenizer.from_list is now deprecated. Create a new Doc object instead and pass in the strings as the `words` keyword argument, for example:\n",
      "from spacy.tokens import Doc\n",
      "doc = Doc(nlp.vocab, words=[...])\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(tokenizer=custom_tokenizer,min_df=10,stop_words=\"english\")\n",
    "X_train_counts = count_vect.fit_transform(input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2344x454 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17368 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params ={\n",
    "    \"C\":np.arange(0.1,2,0.05),\n",
    "    \"kernel\":[\"linear\"]\n",
    "}\n",
    "grid = GridSearchCV(SVC(),params,cv=6,n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 29.60721516609192\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "grid.fit(X_train_tfidf,output_train)\n",
    "print(\"time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.7500000000000002, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.73      0.70      0.71       499\n",
      "          2       0.84      0.37      0.52       278\n",
      "          3       0.71      0.93      0.81      1101\n",
      "          4       0.77      0.51      0.62       466\n",
      "\n",
      "avg / total       0.74      0.73      0.71      2344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masato\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: [W002] Tokenizer.from_list is now deprecated. Create a new Doc object instead and pass in the strings as the `words` keyword argument, for example:\n",
      "from spacy.tokens import Doc\n",
      "doc = Doc(nlp.vocab, words=[...])\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.65      0.58      0.61       125\n",
      "          2       0.60      0.30      0.40        70\n",
      "          3       0.66      0.90      0.76       275\n",
      "          4       0.66      0.37      0.47       117\n",
      "\n",
      "avg / total       0.65      0.65      0.63       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = grid.predict(X_train_tfidf)\n",
    "print(classification_report(output_train,pred))\n",
    "X_test_counts = count_vect.transform(input_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "pred = grid.predict(X_test_tfidf)\n",
    "print(classification_report(output_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.64      0.56      0.60       125\n",
      "          2       0.63      0.37      0.47        70\n",
      "          3       0.66      0.84      0.74       275\n",
      "          4       0.63      0.47      0.54       117\n",
      "\n",
      "avg / total       0.65      0.65      0.64       587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = grid.predict(input_test)\n",
    "print(classification_report(output_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\masato\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: [W002] Tokenizer.from_list is now deprecated. Create a new Doc object instead and pass in the strings as the `words` keyword argument, for example:\n",
      "from spacy.tokens import Doc\n",
      "doc = Doc(nlp.vocab, words=[...])\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "X_test_counts = count_vect.transform(data_test.description)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sub = grid.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['pre'] = pred_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.to_csv('submit_svm4.csv',columns=['id','pre'],header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
